{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7bd03d1-8fe6-4a62-9db7-41ffcdb6eb08",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-11-08T12:25:52.067157Z",
     "iopub.status.busy": "2023-11-08T12:25:52.066488Z",
     "iopub.status.idle": "2023-11-08T12:25:56.343242Z",
     "shell.execute_reply": "2023-11-08T12:25:56.341862Z",
     "shell.execute_reply.started": "2023-11-08T12:25:52.067123Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/ChatGLM-6B-Int4/quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /root/.cache/huggingface/modules/transformers_modules/ChatGLM-6B-Int4/quantization_kernels_parallel.c -shared -o /root/.cache/huggingface/modules/transformers_modules/ChatGLM-6B-Int4/quantization_kernels_parallel.so\n",
      "Kernels compiled : /root/.cache/huggingface/modules/transformers_modules/ChatGLM-6B-Int4/quantization_kernels_parallel.so\n",
      "Load kernel : /root/.cache/huggingface/modules/transformers_modules/ChatGLM-6B-Int4/quantization_kernels_parallel.so\n",
      "Setting CPU quantization kernel threads to 4\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZhipuAI/ChatGLM-6B-Int4\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mZhipuAI/ChatGLM-6B-Int4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m a \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ZhipuAI/ChatGLM-6B-Int4\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"ZhipuAI/ChatGLM-6B-Int4\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()\n",
    "a = model\n",
    "response, history = model.chat(tokenizer, \"使用大模型自动找数\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3862b05b-1ca0-483f-84e1-a391b5c32645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T12:44:52.758093Z",
     "iopub.status.busy": "2023-11-08T12:44:52.757263Z",
     "iopub.status.idle": "2023-11-08T12:45:19.110257Z",
     "shell.execute_reply": "2023-11-08T12:45:19.109378Z",
     "shell.execute_reply.started": "2023-11-08T12:44:52.758061Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'北京和上海是中国最著名和最具代表性城市之一，它们在很多方面都有所不同，以下是其中一些方面：\\n\\n1. 历史和文化背景：北京是中国历史悠久的城市，有着丰富的文化遗产和历史建筑，如故宫、天安门广场、颐和园等。上海则是中国现代史的重要起点，有着外滩、城隍庙、人民广场等标志性建筑和城市景观。\\n\\n2. 气候：北京气候属于温带大陆性季风气候，四季分明，夏季炎热潮湿，冬季寒冷干燥，春秋季节宜人。上海气候属于亚热带季风气候，夏季炎热潮湿，冬季温和多雨，春秋季节也较为宜人。\\n\\n3. 食物：北京是中国美食文化的重要代表之一，以烤鸭、炸酱面、豆汁等为代表的北京特色美食深受游客喜爱。上海则是全球美食文化的中心之一，以火锅、生煎包、小笼包等为代表的上海特色美食也是全球闻名的。\\n\\n4. 城市规划和建筑：北京城市规划和建筑以古代文化和现代建筑相结合为主题，如天安门广场、故宫、颐和园等，上海则以现代建筑和外滩建筑群为代表，是中国最具代表性和现代化的城市之一。\\n\\n北京和上海在历史、文化、气候、食物和城市规划等方面都有很大的不同，希望这些回答能够帮助你更好地了解这两个城市。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import ChatGLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# import os\n",
    "\n",
    "template = \"\"\"{question}\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# default endpoint_url for a local deployed ChatGLM api server\n",
    "endpoint_url = \"http://127.0.0.1:8000\"\n",
    "\n",
    "# direct access endpoint in a proxied environment\n",
    "# os.environ['NO_PROXY'] = '127.0.0.1'\n",
    "\n",
    "llm = ChatGLM(\n",
    "    endpoint_url=endpoint_url,\n",
    "    max_token=80000,\n",
    "    history=[[\"我将从美国到中国来旅游，出行前希望了解中国的城市\", \"欢迎问我任何问题。\"]],\n",
    "    top_p=0.9,\n",
    "    model_kwargs={\"sample_model_args\": False},\n",
    ")\n",
    "\n",
    "# turn on with_history only when you want the LLM object to keep track of the conversation history\n",
    "# and send the accumulated context to the backend model api, which make it stateful. By default it is stateless.\n",
    "# llm.with_history = True\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"北京和上海两座城市有什么不同？\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55218cdd-9f62-4441-8bbf-4bf85886be68",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-10-15T09:04:18.101969Z",
     "iopub.status.busy": "2023-10-15T09:04:18.101206Z",
     "iopub.status.idle": "2023-10-15T09:04:18.106032Z",
     "shell.execute_reply": "2023-10-15T09:04:18.105265Z",
     "shell.execute_reply.started": "2023-10-15T09:04:18.101921Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e6b2d7a-3109-4dcd-8a18-62d21b63f2a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T03:59:41.667285Z",
     "iopub.status.busy": "2023-10-15T03:59:41.666816Z",
     "iopub.status.idle": "2023-10-15T04:03:50.974364Z",
     "shell.execute_reply": "2023-10-15T04:03:50.973696Z",
     "shell.execute_reply.started": "2023-10-15T03:59:41.667255Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 11:59:44,391 - modelscope - INFO - PyTorch version 2.1.0 Found.\n",
      "2023-10-15 11:59:44,394 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2023-10-15 11:59:44,394 - modelscope - INFO - No valid ast index found from /root/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
      "2023-10-15 11:59:44,462 - modelscope - INFO - Loading done! Current index file version is 1.9.2, with md5 6ccf2213f8d5ad11e4a7c2e4d13033d6 and a total number of 941 components indexed\n",
      "2023-10-15 11:59:44,798 - modelscope - INFO - Use user-specified model revision: v1.0.12\n",
      "Downloading: 100%|██████████| 1.29k/1.29k [00:00<00:00, 112kB/s]\n",
      "Downloading: 100%|██████████| 199/199 [00:00<00:00, 18.7kB/s]\n",
      "Downloading: 100%|██████████| 2.28k/2.28k [00:00<00:00, 625kB/s]\n",
      "Downloading: 100%|██████████| 2.30k/2.30k [00:00<00:00, 622kB/s]\n",
      "Downloading: 100%|██████████| 53.5k/53.5k [00:00<00:00, 674kB/s]\n",
      "Downloading: 100%|█████████▉| 1.70G/1.70G [00:27<00:00, 66.0MB/s]\n",
      "Downloading: 100%|█████████▉| 1.83G/1.83G [00:24<00:00, 80.2MB/s]\n",
      "Downloading: 100%|█████████▉| 1.80G/1.80G [00:25<00:00, 76.0MB/s]\n",
      "Downloading: 100%|█████████▉| 1.69G/1.69G [00:25<00:00, 70.6MB/s]\n",
      "Downloading: 100%|█████████▉| 1.83G/1.83G [00:57<00:00, 34.0MB/s]\n",
      "Downloading: 100%|█████████▉| 1.80G/1.80G [00:31<00:00, 60.4MB/s]\n",
      "Downloading: 100%|█████████▉| 0.98G/0.98G [00:11<00:00, 93.1MB/s]\n",
      "Downloading: 100%|██████████| 20.0k/20.0k [00:00<00:00, 761kB/s]\n",
      "Downloading: 100%|██████████| 14.3k/14.3k [00:00<00:00, 445kB/s]\n",
      "Downloading: 100%|██████████| 1.17k/1.17k [00:00<00:00, 302kB/s]\n",
      "Downloading: 100%|██████████| 6.63k/6.63k [00:00<00:00, 1.01MB/s]\n",
      "Downloading: 100%|██████████| 9.82k/9.82k [00:00<00:00, 2.79MB/s]\n",
      "Downloading: 100%|██████████| 995k/995k [00:00<00:00, 6.26MB/s]\n",
      "Downloading: 100%|██████████| 244/244 [00:00<00:00, 68.0kB/s]\n"
     ]
    }
   ],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "model_dir = snapshot_download('ZhipuAI/chatglm2-6b', cache_dir='/mnt/workspace/ChatGLM2-6B', revision='v1.0.12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6505293e-1cc5-4f73-afa6-2b5e28a95d38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T09:14:23.858830Z",
     "iopub.status.busy": "2023-10-15T09:14:23.858102Z",
     "iopub.status.idle": "2023-10-15T09:18:13.936055Z",
     "shell.execute_reply": "2023-10-15T09:18:13.935242Z",
     "shell.execute_reply.started": "2023-10-15T09:14:23.858797Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 17:14:24,175 - modelscope - INFO - Use user-specified model revision: v1.0.1\n",
      "Downloading: 100%|██████████| 838/838 [00:00<00:00, 73.6kB/s]\n",
      "Downloading: 100%|██████████| 161/161 [00:00<00:00, 43.3kB/s]\n",
      "Downloading: 100%|██████████| 4.28k/4.28k [00:00<00:00, 952kB/s]\n",
      "Downloading: 100%|██████████| 2.58M/2.58M [00:00<00:00, 10.4MB/s]\n",
      "Downloading: 100%|██████████| 11.1k/11.1k [00:00<00:00, 3.03MB/s]\n",
      "Downloading: 100%|██████████| 2.29k/2.29k [00:00<00:00, 621kB/s]\n",
      "Downloading: 100%|██████████| 57.0k/57.0k [00:00<00:00, 816kB/s]\n",
      "Downloading: 100%|██████████| 3.64k/3.64k [00:00<00:00, 846kB/s]\n",
      "Downloading: 100%|██████████| 3.63G/3.63G [03:33<00:00, 18.2MB/s]\n",
      "Downloading: 100%|██████████| 28.3k/28.3k [00:00<00:00, 920kB/s]\n",
      "Downloading: 100%|██████████| 1.13k/1.13k [00:00<00:00, 309kB/s]\n",
      "Downloading: 100%|██████████| 1.57k/1.57k [00:00<00:00, 431kB/s]\n",
      "Downloading: 100%|██████████| 3.27k/3.27k [00:00<00:00, 895kB/s]\n",
      "Downloading: 100%|██████████| 16.3k/16.3k [00:00<00:00, 631kB/s]\n",
      "Downloading: 100%|██████████| 446/446 [00:00<00:00, 119kB/s]\n"
     ]
    }
   ],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "model_dir = snapshot_download('ZhipuAI/ChatGLM-6B-Int4', cache_dir='/mnt/workspace', revision='v1.0.1')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
